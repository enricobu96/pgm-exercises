{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Probabilistic Models – Spring 2022\n",
    "## Exercise Session 3\n",
    "Return by Feb 15th 12.00. Session at Feb 15th 14.15.\n",
    "\n",
    "<span style=\"color:red\">**Enrico Buratto**</span>\n",
    "\n",
    "### Instructions\n",
    "Make sure the notebook produces correct results when ran sequentially starting from the first cell. You can ensure this by clearing all outputs, running all cells, and finally correcting any errors.\n",
    "\n",
    "To get points:\n",
    "1. Submit your answers to the automatically checked Moodle test. \n",
    " - You have 5 tries on the test: the highest obtained score will be taken into account.\n",
    " - For numerical questions the tolerance is $1\\cdot10^{-4}$, so you can round the results e.g. up to 5 decimals.\n",
    "2. Submit this notebook containing your derivations to Moodle.\n",
    "\n",
    "The idea in this exercise is to understand how a naive Bayes classifier and a hidden Markov model work, by implementing them yourself. Therefore, you should not use any libraries providing the models off-the-shelf. Also you should not copy-paste any solutions you might find on the Internet. Doing so you will risk losing the points for the exercise. You can, of course, compare the output of your implementation to any reference implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "***\n",
    "\n",
    "Consider the following document corpus, where the first column specifies the class of the document and the second column is the document itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\tText\r\n",
      "spam\tFREE online !!! results\r\n",
      "ham\tresults repository online FREE\r\n",
      "spam\tFREE online results FREE !!! registration\r\n",
      "spam\t!!! registration FREE !!! repository\r\n",
      "ham\tconference online registration conference !!!\r\n",
      "ham\tconference results repository results FREE\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/corpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Construct a multinomial naive Bayes classifier (NBC; p. 25, lecture 5) using the corpus as the training data. Use Laplace smoothing to address the problem of zero probabilities. \n",
    "\n",
    "Report in Moodle:\n",
    "- $P(C=\\text{ham})$,\n",
    "- $P(X=\\text{repository} \\mid C=\\text{spam})$.\n",
    "\n",
    "\n",
    "\n",
    "(b) Use the NBC to calculate the posterior probability for each document that it is spam. \n",
    "\n",
    "Report in Moodle:\n",
    "- the obtained value for the last document in the corpus.\n",
    "\n",
    "(c) Finally, compute and report in Moodle the posterior probabilities for the following documents (not used in the training of the NBC) to be spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\tText\r\n",
      "?\tFREE online conference !!! registration\r\n",
      "?\tconference registration results conference online !!!\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/new_emails.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each question asking for a posterior probability, remember to normalize so that your posterior probabilities sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task a\n",
      "P(C=ham) = 0.5\n",
      "P(X=repository|C=spam) = 0.09090909090909091\n",
      "Task b\n",
      "P(C=spam|D) = 0.11018056221333225\n",
      "Task c\n",
      "P(C=spam|first) = 0.5532194007327792\n",
      "P(C=ham|first) = 0.4467805992672208\n",
      "P(C=spam|second) = 0.1173637534202318\n",
      "P(C=ham|second) = 0.8826362465797682\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # probably using something deprecated but works\n",
    "\n",
    "# take distinct words\n",
    "corpus = pd.read_csv('data/corpus.txt', sep='\\t')\n",
    "words = set()\n",
    "corpus['Text'].str.split().apply(words.update)\n",
    "words = list(words)\n",
    "\n",
    "# count occurrences\n",
    "df = pd.DataFrame(columns=['word','spam','ham'])\n",
    "for word in words:\n",
    "    count_spam = corpus[corpus['Class']=='spam']['Text'].str.count(word).sum()\n",
    "    count_ham = corpus[corpus['Class']=='ham']['Text'].str.count(word).sum()\n",
    "    df = df.append({'word': word, 'spam': count_spam, 'ham': count_ham}, ignore_index=True)\n",
    "\n",
    "# count probabilities (+1 for laplacian smoothing)\n",
    "df['spam'] = (df['spam'] + 1)/(df['spam'].sum() + len(words))\n",
    "df['ham'] = (df['ham'] + 1)/(df['ham'].sum() + len(words))\n",
    "\n",
    "# Task a\n",
    "print('Task a')\n",
    "print('P(C=ham) = 0.5') # this is just number of ham/total\n",
    "print('P(X=repository|C=spam) =', df[df['word']=='repository']['spam'].values[0])\n",
    "\n",
    "# Task b\n",
    "\n",
    "def calculatePosterior(data, df):\n",
    "    last = data\n",
    "    spam_given_d = 1\n",
    "    for word in last.split():\n",
    "        spam_given_d *= df[df['word']==word]['spam'].values[0]\n",
    "    spam_given_d *= .5\n",
    "    ham_given_d = 1\n",
    "    for word in last.split():\n",
    "        ham_given_d *= df[df['word']==word]['ham'].values[0]\n",
    "    ham_given_d *= .5\n",
    "    return spam_given_d/(spam_given_d+ham_given_d)\n",
    "\n",
    "print('Task b')\n",
    "print('P(C=spam|D) =', calculatePosterior(corpus['Text'].iloc[-1], df))\n",
    "\n",
    "# Task c\n",
    "test = pd.read_csv('data/new_emails.txt', sep='\\t')\n",
    "\n",
    "# first one\n",
    "print('Task c')\n",
    "post = calculatePosterior(test['Text'].iloc[0], df)\n",
    "print('P(C=spam|first) =', post)\n",
    "print('P(C=ham|first) =', 1-post)\n",
    "\n",
    "post = calculatePosterior(test['Text'].iloc[1], df)\n",
    "print('P(C=spam|second) =', post)\n",
    "print('P(C=ham|second) =', 1-post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "***\n",
    "\n",
    "**NB**: The exercise does not assume knowledge on molecular biology. The description shows one simplified use case for a hidden Markov model.\n",
    "\n",
    "The central dogma of molecular biology (roughly) states that DNA is transcribed into RNA. The RNA is then translated into protein, and proteins perform most of the functions within the cell. A protein is composed of a string of amino acids. While there are about twenty commonly occuring amino acids, and they can be grouped many different ways, we will consider them all to belong to either the Aliphatic or Hydroxyl group (to simplify calculations). This sequence is called the primary structure of the protein. A key factor in understanding the function of a particular protein is its secondary structure, that is, how the amino acids behave in three dimensional space. For example, the following figure shows a cartoon rendering of the secondary structure of a protein.\n",
    "\n",
    "![](secondarystructure.jpg)\n",
    "\n",
    "As the figure suggests, two of the most important features of the seconary structure are $\\alpha$ helices and $\\beta$ sheets. However, it is difficult to assess the secondary structure without expensive experimental techniques such as crystallography. On the other hand, finding the sequence of amino acids in a protein sequence is inexpensive. Consequently, a goal of computational biology is to infer the secondary structure of a protein given its primary structure (the sequence of amino acids).\n",
    "\n",
    "Suppose we conduct a set of costly crystallography experiments and discover the following secondary structures and associated amino acid sequences. $\\alpha$ means $\\alpha~\\text{helix}$, $\\beta$ means $\\beta~\\text{sheet}$, $\\mathtt{A}$ means aliphatic and $\\mathtt{H}$ means hydroxyl. For example, in the first protein (i.e., first row) the second $\\beta$ corresponds to the third $\\mathtt{A}$, because they are both in the third position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "αββααββ AAAHHHA\r\n",
      "αααβααβ HHAAHHH\r\n",
      "βαααββ AAAAHA\r\n",
      "αββαααβα AHAAAHAA\r\n"
     ]
    }
   ],
   "source": [
    "!cat data/sequences.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Use these sequences to calculate the parameters for a hidden Markov model (HMM) in which the state variables correspond to the secondary structures and the observations correspond to the amino acids. We will use $S_i$ and $O_i$ to denote the secondary structure and the amino acid in position $i$, respectively. Recall that for an HMM, we need:\n",
    "\n",
    "- The prior probabilities for the first state, $S_1$.  In this case, use the observed counts of the first state in each sequence.\n",
    "- The *emission* probabilities, that is, the probability of observing a particular amino acid given the secondary structure.\n",
    "- The *transition* probabilities, that is, the probability of the next state given the current state.\n",
    "\n",
    "In all cases, use Laplace smoothing, as was done in Exercise 1.\n",
    "\n",
    "Report in Moodle:\n",
    "\n",
    "- the prior probability $P(S_1 = \\alpha)$,\n",
    "- the emission probabilities $P(O_i = \\mathtt{A} \\mid S_i = \\alpha)$ and $P(O_i = \\mathtt{H} \\mid S_i = \\beta)$,\n",
    "- the transition probabilities $P(S_{i+1} = \\beta \\mid S_i = \\alpha)$ and $P(S_{i+1} = \\alpha \\mid S_i = \\beta)$.\n",
    "\n",
    "(b) Now, using the HMM, use the foward-backward algorithm to calculate the posterior probabilities over all the secondary structure states, given the following observed amino acid sequence:\n",
    "\n",
    "- $\\mathtt{AHHAAHAAHA}$\n",
    "\n",
    "Report in Moodle:\n",
    "- $P(S_6 = \\alpha \\mid O_{1:10})$, i.e., the posterior probability for the 6th secondary state (corresponding to the third $\\mathtt{H}$ in the sequence) to be $\\alpha$.\n",
    "\n",
    "(c) Finally, implement and use the Viterbi algorithm to calculate the most likely sequence of secondary structure states for the sequence of observations used in (b).\n",
    "\n",
    "Report in Moodle:\n",
    "- The obtained sequence. If there are multiple sequences with equally high likelihood, report any one of them. To avoid possible problems with character encoding use a and b to denote $\\alpha$ and $\\beta$: if you obtain the sequence $\\alpha\\alpha\\alpha\\alpha\\alpha\\beta\\beta\\beta\\beta\\beta$ report it as aaaaabbbbb.\n",
    "\n",
    "Again, remember to normalize so that the posterior probabilities sum up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your answer in cells here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
